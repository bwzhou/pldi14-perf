\section{Extensions to \lancet}
\label{sec:extensions}

This section describes two extensions to \lancet that allow it to support a
broader range of programs. First, we discuss how \lancet handles programs that
have multiple symbolic variables that determine the program's scale. Second,
we discuss how we can deal with programs that have structured input, rather
than just programs that accept numeric input.

\subsection{Handling multiple symbolic variables}
\label{sec:multivar}

Up until now, our discussion of \lancet has assumed that there is but a single
symbolic variable in a program that controls the scale of execution. In other
words, the procedure for generating scaling constraints has focused on
inferring the necessary constraints for a single symbolic variable. However,
in many programs, the number of iterations of a particular loop may be
controlled by multiple input parameters, rather than just one. Consider, for example, a modified version of the example in Figure~\ref{fig:example}, where in line~\ref{line:u}, instead of $u$ being defined as 10, its value is read from the user ({\em i.e.}, $u$ is also symbolic). By making $u$ symbolic, the trip count of the inner loop (at line~\ref{line:innerloop}) is $u \times v1$.

A loop whose trip count is controlled by more than one symbolic variable
presents a problem for \lancet, because there are {\em multiple degrees of
freedom} when trying to scale up a loop: to run the inner loop additional
times, either $u$ or $v1$ could be increased. To see why this is a problem,
consider how multiple degrees of freedom affect the generation of the training
set for constraint inference. To run the loop 10 times, \lancet might generate
the constraints $(u = 10 \land v1 = 1)$, while to run the loop 20 times, \lancet
might generate the constraints $(u = 4 \land v1 = 5)$, and to run the loop 30
times, the constraints $(u = 15 \land v1 = 2)$. There is no clear trend in the
constraints on either $u$ or $v1$; if \lancet tried to generate constraints to
run the loop 1000 times, its statistical inference would produce invalid
results. Essentially, whenever there are multiple degrees of freedom, \lancet
is less likely to produce a clear trend in constraints that can be exploited
for inference.

To avoid these problems, we restrict \lancet to performing inference when there
is only a single degree of freedom; in other words, {\em \lancet only attempts
to use one symbolic variable to scale up a program}. This seems to present a
problem when using \lancet for general programs, since most programs have more
than one input parameter, and hence more than one symbolic variable that
\lancet could control. \lancet handles such programs using a slight modification
of the basic procedure outlined in Section~\ref{sec:method}. First, \lancet
generates training inputs {\em with all input parameters left as symbolic}.
While this may not yield clear trends for scaling constraints, the constraint
sets generated in this way can still be used to identify validity constraints:
any constraint on {\em any} symbolic variable that is common across the
scales is considered a validity constraint.

Having generated validity constraints, \lancet now iteratively selects a {\em
scaling set}, a set of symbolic variables that will be allowed to change with
scale. Any symbolic variables that are related by {\em equality} constraints
constitute just a single degree of freedom, since the variables are completely
dependent on each other, and so are placed into a single scaling set. Symbolic
variables that are unrelated, or only related by inequality constraints, can
vary independently, and so are placed in separate scaling sets. For a given
scaling set, \lancet uses its SMT solver to generate valid, {\em concrete}
values for all variables that are not in the scaling set, while leaving the
variables in the scaling set symbolic. For example, in
Figure~\ref{fig:example}, with $u$ symbolic, there are two scaling sets: $\{u\}$ and
$\{v1, v2\}$, so \lancet would choose $u$ to remain symbolic while concretizing $v1$ and $v2$,
making sure that the validity constraints are met. Whenever possible, a
symbolic variable is concretized as 1, to ensure maximum flexibility when
scaling a loop (note, for example, that if $v1$ were concretized as 3, it would
be impossible to run the inner loop exactly 10 times). With all symbolic
variables except one concretized, \lancet can use the basic approach described
before to scale up a loop. Note that this process must re-compute the
constraint sets for the training runs, to ensure that all training inputs are
generated by varying just one symbolic variable. \lancet then repeats this
procedure for each scaling set, generating inputs that scale up the loop in different ways (\eg increasing $v1$ scales the loop by increasing the number of columns in the matrix, while increasing $u$ scales the loop by increasing the number of rows).

We note that this approach limits \lancet's generality: some programs might
{\em require} changing multiple variables to scale up a loop, while other
programs may have validity constraints on symbolic variables that prevent them
from being concretized correctly for all scales (\eg if a program has
the constraints $(y < x \land y > x/2)$, there is no concrete value of $x$
that allows $y$ to be freely scaled up or down). In such scenarios, \lancet
will fail to generate scaling inputs. In our experiments, we have yet to
encounter such a scenario, but addressing this shortcoming is an important
avenue of future work.

\subsection{Handling structured input}
\label{sec:structured}

% \note{Are we actually going to implement this? If we don't have any examples
% that use this, we should drop this section.}

Typical dynamic symbolic execution techniques do not distinguish between
simple, numeric input ({\em e.g.}, integer input parameters) and {\em
structured input} ({\em e.g.}, an input text file). Because the input is
generated from a specific dynamic run of the program, with symbolic variables
that are concretized as necessary, every part of the input to a program can
be directly modeled by the symbolic execution tool. For example, in a
word-count program, each {\em character} of the input file can be modeled as a
separate input variable. If a larger input file is necessary, the tool simply
generates a new input variable to represent the next character, with that
character being constrained to be either whitespace or a regular character,
depending on whether the branch detecting the end of the word is taken or not
taken.

Because \lancet generates large-scale inputs without the luxury of a backing
dynamic execution, handling structured input is challenging. While \lancet can
infer numerical constraints over input variables, allowing it to scale up a
program controlled by numerical parameters, scaling up a program that deals
with structured input is less straightforward. To see why this is intuitively,
consider a program such as SPEC CPU2006's 403.gcc, which operates over C input
files. To infer the constraints for a large scale input, \lancet would somehow
have to infer what a large, syntactically-correct C program looks like!

In general, handling structured input requires some programmer intervention.
We define structured input as variable-length input whose {\em contents}
affect program behavior ({\em e.g.}, an input array where the values of the
array elements affect the program, not just the size of the array). \lancet
understands the semantics of, and can perform inference over, scalar variables
such as integers, floats and characters, but does not understand the semantics
of arrays, text files, etc. If input is structured, then \lancet cannot infer
what a larger-scale input might look like.

If a program uses such structured input, \lancet requires the use of a {\em
generator shim.} A generator shim is a small utility program that, when given
numerical input(s) generates a properly-structured input file for its target
program. For example, a generator shim for arrays might take the size of the
array as input and generate an array of the specified size with random values
for each of its entries. The generator shims might be application-specific: a
generator shim for a word-count program could produce an input with a
specified number of distinct words, while a shim for 403.gcc might generate a
C file with a specified number of statements.

\lancet uses generator shims as follows. A generator shim is essentially a
function $g: \mathbb{Z}^k \rightarrow S$ that takes a series of input values
and produces a structured input,\footnote{For illustration, the inputs are
considered to be integers, but can be any scalar values.} while the original
program is essentially a function $p: S \rightarrow O$ that takes a structured
input and produces an output. By combining the generator shim with the
program, \lancet sees a combined program $p': \mathbb{Z}^k \rightarrow O$, a
program that accepts a fixed number of numeric inputs to produce an output.
\lancet can now analyze and perform inference over this combined program,
generating inputs to the shim that will generate a structured input that can
make the target loop in the original program run for the specified number of
iterations. By then running the shim alone, the programmer can generate the
actual large-scale input.

\lancet provides a set of basic generator shims for arrays and text files that
fill the generated data structure with random values. Programmers can also
define their own shims as necessary. Providing a more general approach for
\lancet to handle structured input, and minimizing the use of
programmer-provided shims, is another avenue of future work.
