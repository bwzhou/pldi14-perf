\section{Discussion}\label{sec:discussion}

This section elucidates some of the design decisions of \lancet, and discusses some of the scenarios under which it will fail to correctly generate large-scale inputs.

\begin{figure}
  \centering
  \begin{lstlisting}
    int N = /* read */
    
    [[target_loop]]
    for (int i = 0; i < N; i += 10)
      /* do something */
  \end{lstlisting}
  \vspace{-5pt}
  \caption{Example code that underconstrains inputs}
  \label{fig:underconstrained}
\end{figure}

One interesting question is why, given a number of small-scale inputs, \lancet does not perform inference directly over the inputs to generate a large-scale input, choosing instead to perform inference over the path conditions. We choose this approach because the path condition for a given loop trip count often {\em underconstrains} the input; more than one concrete input will cause the target loop to iterate the correct number of times. In such a scenario, it is entirely possible that the path conditions exhibit a simple, predictable scaling trend, but the concrete inputs generated by the constraint solver do not. Consider the loop in Figure~\ref{fig:underconstrained}. The path constraint for the loop to run $k$ times is: $(N > 10*k - 10) \land (N \le 10*k)$. Valid concrete values of N that would run the loop 1, 2, 3 or 4 times are 10, 11, 30 and 31, respectively, which do not yield a clear trend. The trends in the constraints, however, are much more clear, and \lancet can correctly build a model of the path constraints.

Even though \lancet performs inference over constraints to avoid issues with underconstrained inputs, the problem can still arise when dealing with constraints. The meta-constraint that \lancet uses requires that the path condition traverse a loop a certain number of times. But what if multiple path conditions can satisfy that meta-constraint? We have already described how \lancet handles some instances of this problem in Section~\ref{sec:multivar}: programs with multiple symbolic variables often have meta-constraints that underconstrain the path condition, so by focusing on just one symbolic variable, the problem can be somewhat ameliorated. In general, however, \lancet may encounter weak meta-constraints, and hence generate small-scale path constraints that do not evidence a clear scaling trend. A similar issue arises when, as discussed in Section~\ref{sec:constraint-inference}, different constraint features appear in different path conditions, preventing \lancet from detecting any trend.

In such scenarios, \lancet, rather than making a poor prediction for a given scaling constraint, errs on the side of making {\em no} prediction: it drops constraint features that are not common across training path conditions, and does not make predictions for scaling features that are poorly predicted (the accuracy of prediction can be estimated through cross-validation of the statistical model). As a result, the inferred path condition may, itself, be underconstrained: it is no longer guaranteed that an input satisfying the path condition will satisfy the meta-constraint. We note, however, that \lancet consistently errs on the side of generating underconstrained path constraints: dropping clauses when inferring path conditions results in a weaker constraint, and does not prevent a valid input from satisfying the path constraint. Hence, this limitation primarily results in \lancet suggesting {\em too many} inputs for large-scale runs, rather than missing a potentially valid one.
